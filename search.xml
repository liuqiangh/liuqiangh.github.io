<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>attention-based  model</title>
    <url>/2020/11/12/attention-based%20%20model/</url>
    <content><![CDATA[<h4 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h4><p>RNN和LSTM中时间记忆不会很长，因为记忆越长，参数越多，训练很困难，但attention机制参数不会增加。</p>
<h4 id="machine-translation"><a href="#machine-translation" class="headerlink" title="machine translation"></a>machine translation</h4><p><strong>sequence to sequence</strong> </p>
<ul>
<li>RNN实现</li>
</ul>
<img src="https://i.loli.net/2020/05/19/AWs6VIeyHawxO5J.png" style="zoom: 50%;" />

<p>缺点是图中红框绿背景代表的vector可能已经将学习到的整个句子的信息忘记掉</p>
<ul>
<li>attention-based  model实现</li>
</ul>
<img src="https://i.loli.net/2020/05/19/X6Y7GLlREZtc3U5.png" style="zoom:50%;" />

<div align=left>

<p>$\alpha_0^1$是$h_0^1$对应的权重，算出所有权重后会进行softmax和加权，得到$c_0$</p>
<img src="https://i.loli.net/2020/05/19/pQNW3oiIOsUH8m6.png" style="zoom:50%;" />

<img src="https://i.loli.net/2020/05/19/VFjKL48hoeZuQnS.png" style="zoom:50%;" />

<p>可以看到Encoding和decoding阶段仍然是rnn，但是decoding阶段使用attention的输出结果$c_0,c_1$作为rnn的输入。</p>
<p>权重$\alpha$的常用计算方法有三种：</p>
<img src="https://i.loli.net/2020/05/20/MpdBLrjyah6X5IN.png"  />

<p>思想就是根据当前解码“状态”判断输入序列的权重分布。</p>
<img src="https://i.loli.net/2020/05/19/VtIsTdRQXNczwiK.png" style="zoom:50%;" />





<p>attention模型每次attention的位置由机器确定。</p>
<p>如果把attention剥离出来去看的话，其实就是如下机制：</p>
<img src="https://i.loli.net/2020/05/20/vfoR2A8SsJVGl7a.png" style="zoom: 80%;" />

<p>输入是query(Q),key(K),value(V)，输出是attention value。如果与之前的模型对应起来的话，query就是$z_0,z_1$，key就是$h_1,h_2,h_3,h_4$，value也是$h_1,h_2,h_3,h_4$。模型通过Q和K的匹配计算出权重，再结合V得到输出：<br>$$<br>Attention(Q,K,V)=softmax(sim(Q,K))V<br>$$<br><strong>neural turing machine</strong></p>
<img src="https://i.loli.net/2020/05/19/GhxyMOvlAgUpnau.png" style="zoom:50%;" />

<img src="https://i.loli.net/2020/05/19/UySNkgTIOiPoFlR.png" style="zoom:50%;" />

<img src="https://i.loli.net/2020/05/19/gBdARTcSqQFuViM.png" style="zoom:50%;" />]]></content>
      <categories>
        <category>自学</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>Bert原理解读</title>
    <url>/2020/11/12/Bert/</url>
    <content><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><h4 id="rnn问题"><a href="#rnn问题" class="headerlink" title="rnn问题"></a>rnn问题</h4><img src="https://i.loli.net/2020/05/20/cSCL62qKPBJx7we.png" style="zoom: 33%;" />

<p>后一层的隐层数据需用到前一层的隐层参数，导致无法并行计算</p>
<p>由此引出transformer,其中重点是注意力机制：attention-based model</p>
<h4 id="传统word2vec问题"><a href="#传统word2vec问题" class="headerlink" title="传统word2vec问题"></a>传统word2vec问题</h4><p>预训练好的向量会永久不变，但实际上在不同语境中同一个词会有不同的意思，bert可以解决这个问题</p>
<h3 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h3><h4 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h4><p>模型自己可以找到一句话的重点，例如下图中，第一个句子“it”代指“animal”，第二个“it”代指“street”，要达到这样的效果，模型需要注意到该词的上下文语境，给不同的上下文词汇以不同的权重。</p>
<img src="https://i.loli.net/2020/05/20/QPULyjo9gr7dwMz.png" style="zoom:50%;" />

<p>假如一个句子有n个词，第一个词定义一个查询向量$q_1$，要去查询$q_1$和$k_1,k_2,…,k_n$之间的关系，而$v_1,v_2,…,v_n$代表n个词本身的特征向量。</p>
<p><img src="https://i.loli.net/2020/05/20/iLOKzdTwCa6UySP.png" style="zoom:50%;" /><img src="https://i.loli.net/2020/05/20/gZLXuQmCxklNTWJ.png" alt=""></p>
<img src="https://i.loli.net/2020/05/20/gZLXuQmCxklNTWJ.png" style="zoom:50%;" />

<p>$q_1$和$k_i$内积越大，表示相关性越高，表示对该词的影响越大，”注意力”在第i个词上越多，即最终权重越大。</p>
<p>$d_k$代表维度，因为维度越大，内积越大，而实际上不代表维度越大，影响越大，因此要除去维度的影响。</p>
<img src="https://i.loli.net/2020/05/20/wK41fLMaIelENOU.png" style="zoom:50%;" />

<p>抽丝剥茧，可简化为如下流程：</p>
<img src="https://i.loli.net/2020/05/20/VfDQZm8obBntdws.png" style="zoom:50%;" />

<p>整体计算流程如下</p>
<img src="https://i.loli.net/2020/05/20/W97Lu4UXHFiqyn3.png" style="zoom:50%;" />

<h4 id="multi-headed机制"><a href="#multi-headed机制" class="headerlink" title="multi-headed机制"></a>multi-headed机制</h4><p>类似卷积神经网络里有多个卷积核，这里有多个头，提取多个特征表达。</p>
<img src="https://i.loli.net/2020/05/20/fO7dZl1zn6PyAe5.png" style="zoom:50%;" />

<ul>
<li>通过不同的head得到多个特征表达（一般8个头）</li>
<li>将所有特征拼接在一起</li>
<li>可以通过再一层全连接来降维</li>
</ul>
<img src="https://i.loli.net/2020/05/20/Yw9VrhXWKGDOuat.png" style="zoom:50%;" />

<p>整体结构如下图</p>
<img src="https://i.loli.net/2020/05/20/mrxZ8W6ETVzDyLq.png" style="zoom:50%;" />

<h4 id="堆叠多层"><a href="#堆叠多层" class="headerlink" title="堆叠多层"></a>堆叠多层</h4><p>进行多次self-attention</p>
<img src="https://i.loli.net/2020/05/20/uy1XxN27OQiVZYe.png" style="zoom:50%;" />

<h4 id="位置信息表达"><a href="#位置信息表达" class="headerlink" title="位置信息表达"></a>位置信息表达</h4><p>在self-attention中每个词都会考虑整个序列的加权，所以其出现位置并不会对结果产生什么影响，相当于放哪都无所谓，但是这跟实际就有些不符合了，我们希望模型能对位置有额外的认识。</p>
<img src="https://i.loli.net/2020/05/20/YqAjSFfhdElcXC3.png" style="zoom:50%;" />

<p>位置信息编码有很多种，论文原文采用正弦余弦周期表达（开源代码已提供）。</p>
<h4 id="Add与Normalize"><a href="#Add与Normalize" class="headerlink" title="Add与Normalize"></a>Add与Normalize</h4><ul>
<li>layer层做normalization使训练速度更快，更稳定，NLP一般采用layer normalization(相当于在句子之间进行了归一化)</li>
<li>残差连接，防止一顿操作后梯度消失（多手准备，至少不比原来差）</li>
</ul>
<img src="https://i.loli.net/2020/05/20/loa2pTmrA8KPYkL.png" style="zoom:50%;" />

<h4 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h4><ul>
<li>decoder端加入了encoder-decoder attention</li>
<li>mask机制，output里预测第n个词时，不能使用第n个词后面的词，因此要把后面的词mask掉。</li>
</ul>
<img src="https://i.loli.net/2020/05/20/XcOvKxqghBDuCaY.png" style="zoom:50%;" />

<h4 id="最终结果"><a href="#最终结果" class="headerlink" title="最终结果"></a>最终结果</h4><img src="https://i.loli.net/2020/05/20/8IxegUsuyJb7CZo.png" style="zoom:50%;" />

<p>类似cnn的分类任务，应该接交叉熵函数。</p>
<img src="https://i.loli.net/2020/05/20/bptgX59xQR8Nied.png" style="zoom:50%;" />

<h3 id=""><a href="#" class="headerlink" title=""></a></h3>]]></content>
      <categories>
        <category>自学</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP-word2vec</title>
    <url>/2020/11/12/NLP-word2vec/</url>
    <content><![CDATA[<p>输入的词的向量先进行初始化，在训练过程中输入和中间层参数也会更新，模型内部就是神经网络</p>
<h4 id="构建数据"><a href="#构建数据" class="headerlink" title="构建数据"></a>构建数据</h4><p>窗口滑动</p>
<div align=center>

<img src="https://i.loli.net/2020/05/21/1FsqhLytp3JlCSz.png" style="zoom:50%;" />

<h4 id="不同模型的对比"><a href="#不同模型的对比" class="headerlink" title="不同模型的对比"></a>不同模型的对比</h4><p><strong>CBOW</strong></p>
<p>输入是上下文，输出是中间的词，图中例子窗口是5。</p>
<p><strong>skipgram</strong></p>
<p>输入是中间此，输出是上下文，图中窗口是5。</p>
<img src="https://i.loli.net/2020/05/21/3QLMqj2Xuc4A6Ta.png" style="zoom: 80%;" />

<p><img src="https://i.loli.net/2020/05/21/RZMASP8j5Ttk6Nq.png" style="zoom:50%;" /><img src="https://i.loli.net/2020/05/21/oJ6sNwkfZa7KdPI.png" alt=""></p>
<p>如何训练</p>
<p>如果一个语料库稍微大点儿，可能的结果很多，最后一层softmax，计算起来十分耗时，如何解决？</p>
<p>以skipgram为例，将输出也放在输入，判断这两个词是不是上下文 ，这样的话标签都是1，不好训练，要自己构建负采样的样本，变成2分类任务 。</p>
<img src="https://i.loli.net/2020/05/21/awIRMFipzWvd53b.png" style="zoom:80%;" />

<p><img src="https://i.loli.net/2020/05/21/NwsurOMTXfW2jUi.png" alt=""></p>
<img src="https://i.loli.net/2020/05/21/EU9dbjWVznQZiyJ.png" style="zoom:50%;" />

<p><img src="https://i.loli.net/2020/05/21/1DU9jdo8gEzxc3k.png" alt=""></p>
<p>构建模型过程中，对于一些不常出现的词（在语料库中出现频率非常低）可以用[unk]表示</p>
<p>可视化展示可以利用tensorboard</p>
]]></content>
      <categories>
        <category>自学</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>RNN</title>
    <url>/2020/11/12/RNN/</url>
    <content><![CDATA[<p>在使用过程中，最终结果只取隐层最后时刻的值，因为最后时刻的值包含了前面时刻隐层的特征</p>
<p>RNN会把t时刻前的所有时刻的特征记住，而如果序列太长，最前面的时刻特征对当前时刻可能没有用，所以引入LSTM，选择性遗忘一些内容</p>
<p>RNN的输入[batch_size,timestep,feature_size]，time_step表示时间序列长度，feature_size表示每个输入的特征向量</p>
<p>label要用one-hot编码</p>
<p>一般LSTM两三层</p>
<h2 id="tf-nn-bidirectional-dynamic-rnn"><a href="#tf-nn-bidirectional-dynamic-rnn" class="headerlink" title="tf.nn.bidirectional_dynamic_rnn"></a>tf.nn.bidirectional_dynamic_rnn</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.nn.bidirectional_dynamic_rnn(</span><br><span class="line"> </span><br><span class="line">    cell_fw,</span><br><span class="line">    cell_bw,</span><br><span class="line">    inputs,</span><br><span class="line">    sequence_length=None,</span><br><span class="line">    initial_state_fw=None,</span><br><span class="line">    initial_state_bw=None,</span><br><span class="line">    dtype=None,</span><br><span class="line">    parallel_iterations=None,</span><br><span class="line">    swap_memory=False,</span><br><span class="line">    time_major=False,</span><br><span class="line">    scope=None</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p><strong>参数：</strong></p>
<p>cell_fw:一个RNNCell实例，用于前向。</p>
<p>cell_bw: 一个RNNCell实例，用于后向。</p>
<ul>
<li>Inputs:RNN的输入。如果time_major=False(默认)，张量的形状为：[batch_size,max_time,…]或这些元素的嵌套的tuple。如果time_major==True，张量的形状为：[max_time,batch_size,…]或者这些元素的嵌套的tuple。也可能是满足此属性的Tensors（可能是嵌套的）元组。每个时间步的单元输入的是张量或者是维度为[batch_size,…]的张量的tuple</li>
<li>sequence_length:可选参数。一个大小为[batch_size]的int32/int64类型向量。表示每个输入样本长度，如时间步长。更多的考虑到性能而不是正确性。</li>
<li>Initial_state:_fw：可选参数。一个针对前向RNN的初始状态。[batch_size, cell_fw.state_size]</li>
<li>Initial_state_bw: 可选参数。一个针对后向RNN的初始状态。[batch_size, cell_fw.state_size]</li>
<li>dtype:初始状态和预期输出的数据类型。</li>
<li>parallel_iteratioins:默认为32。并行运行的迭代次数。用于那些没有任何时间依赖性并且可以并行运行的操作。该参数是使用空间换时间。值&gt;1则使用更多内存但占用更少时间。使用较小的值则使用较少内存但计算时间较长。</li>
<li>swap_memory:透明地交换前向推理中产生的张量，但对于后向传播，需要从GPU到CPU。这允许训练通常不适合单个GPU的RNN，具有非常小的性能损失。</li>
<li>time_major:指定输入和输出张量的形状格式。如果为True，张量必须形如[max_time,batch_size,depth].如果为False，张量必须形如：[batch_size,max_time,depth]。使用time_major=True会更有效，因为它避免了RNN计算开始和结束时的转置。但是，大多数Tensorflow数据都是batch_size为major的数据，因此，默认情况下，此函数以batch-major形式接受输入和输出。</li>
<li>scope:用于创建子图的VariableScope，默认是”rnn”。</li>
</ul>
<p><strong>返回：</strong></p>
<p>一个(outputs,output_states)元组，其中：</p>
<ul>
<li>outputs:一个(output_fw,output_bw)元组，包含前向和后向rnn输出的张量。如果time_major=False(默认的),output_fw是一个形如[batch_size,max_time,cell_fw.output_size]的张量，cell_fw.output_size即为前向隐藏层深度，output_bw是一个形如[batch_size,max_time,cell_bw.output_size]的张量，cell_bw.output_size即为后向隐藏层深度。如果time_major==True，output_fw是一个形如[max_time, batch_size,cell_fw.output_size]的张量，output_bw是一个形如[max_time, batch_size,cell_bw.output_size]的张量。与bidirectioinal_rnn不同，它返回一个元组而不是单个连接的Tensor。如果首选连接的，则前向和后向输出可连接为tf.concat(outputs,2)。</li>
<li>output_states:一个(output_state_fw,output_state_bw)的元组，包含了前向和后向最后的隐藏状态的组成的元组。<br><code>output_state_fw</code>和<code>output_state_bw</code>的类型为LSTMStateTuple。LSTMStateTuple由（c，h）组成，分别代表memory cell和hidden state。</li>
</ul>
<p><strong>如果cell 是 LSTM：</strong></p>
<p>​    state在单层rnn中，输出包含了最后一层的，前向和后向C和H，两者代表的都是最后层的最后一时刻的输出，前后向的H，对应的output的最后一个时刻值；</p>
<p>　　state在多层rnn中，输出包含了所有层的的，前向和后向C和H，两者代表的都是每一层的最后一时刻的输出，前后向的H，对应的output每一层的最后一个时刻值（output 只保存最后一层的输出，state 保存所有层的 H和C）；</p>
<p><strong>如果cell 是 GRU：</strong></p>
<p> 　state在单层rnn中，输出包含了最后一层的，前向和后向的H，两者代表的都是最后层的最后一时刻的输出，前后向的H，对应的output的最后一个时刻值；</p>
<p>　　state在多层rnn中，输出包含了所有层的，前向和后向的H，H代表的是每一层的最后一时刻的输出，前后向的H，对应的output每一层的最后一个时刻值（output 只保存最后一层的输出，state 保存所有层的 H）；</p>
]]></content>
      <categories>
        <category>自学</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorflow使用注意要点</title>
    <url>/2020/11/12/tensorflow%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F%E8%A6%81%E7%82%B9/</url>
    <content><![CDATA[<ol>
<li><p>创建图后返回的变量名和定义模型时的变量名不能一样，否则会报错，即如下代码是会报错的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss, accuracy = sess.run([loss, accuracy], feed_dict=&#123;x: x_batch, y: y_batch, l: l_batch&#125;)</span><br></pre></td></tr></table></figure>

<p>上述代码会导致第二次运行时，会将第一次得到的值（左侧的loss）直接赋值给右侧，会与之前定义的冲突，修改成如下即可。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_loss, train_accuracy = sess.run([loss, accuracy], feed_dict=&#123;x: x_batch, y: y_batch, l: l_batch&#125;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>代码中有使用tf.get_variable和tf.variable创建变量，则在创建图后要对变量进行统一的初始化，即使用该函数tf.global_variables_initializer()。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure>

</li>
</ol>
]]></content>
      <categories>
        <category>自学</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>python系列笔记2</title>
    <url>/2020/11/12/python%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B02/</url>
    <content><![CDATA[<ol>
<li><p>np.random.permutation</p>
<p>随机排列序列，比如</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a=np.random.permutation(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出结果</span></span><br><span class="line">[<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">4</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>tf.device()</p>
<p>在tensorflow中指定模型运行的具体设备，可以指定在CPU或者哪块儿GPU上运行。比如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/gpu:1'</span>): <span class="comment">#指定在第二块gpu上运行</span></span><br><span class="line"><span class="comment"># with tf.device('/cpu:0'):  #指定在cpu上使用CPU不区分设备号，统一使用 /cpu:0</span></span><br><span class="line">    v1 = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], shape=[<span class="number">3</span>], name=<span class="string">'v1'</span>)</span><br><span class="line">    v2 = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], shape=[<span class="number">3</span>], name=<span class="string">'v2'</span>)</span><br><span class="line">    sumV12 = v1 + v2</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.Session(config=tf.ConfigProto(log_device_placement=<span class="literal">True</span>)) <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="keyword">print</span> sess.run(sumV12)</span><br></pre></td></tr></table></figure>

<p>ConfigProto() 中参数 <strong><em>log_device_placement=True\</em></strong> 会打印出执行操作所用的设备，以上输出:</p>
<img src="https://img-blog.csdn.net/20180329194542470" alt="img"  />
</li>
<li><p>tf.reduce_max</p>
<p>Computes the maximum of elements across dimensions of a tensor. (deprecated arguments)，计算最大值，该函数有如下参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.reduce_max(</span><br><span class="line">    input_tensor,</span><br><span class="line">    axis=<span class="literal">None</span>,</span><br><span class="line">    keepdims=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    reduction_indices=<span class="literal">None</span>,</span><br><span class="line">    keep_dims=<span class="literal">None</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<p>使用举例如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a=np.array([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">            [<span class="number">5</span>, <span class="number">3</span>],</span><br><span class="line">            [<span class="number">2</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">b = tf.Variable(a)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(b))</span><br><span class="line">    print(<span class="string">'************'</span>)</span><br><span class="line">    <span class="comment"># 对于二维矩阵，axis=0轴可以理解为行增长方向（向下）,axis=1轴可以理解为列增长方向(向右）</span></span><br><span class="line">    print(sess.run(tf.reduce_max(b, axis=<span class="number">1</span>, keepdims=<span class="literal">False</span>)))  <span class="comment"># keepdims=False,axis=1被消减</span></span><br><span class="line">    print(<span class="string">'************'</span>)</span><br><span class="line">    print(sess.run(tf.reduce_max(b, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)))</span><br><span class="line">    print(<span class="string">'************'</span>)</span><br><span class="line">    print(sess.run(tf.reduce_max(b, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)))</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[[<span class="number">1</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">5</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">6</span>]]</span><br><span class="line">************</span><br><span class="line">[<span class="number">2</span> <span class="number">5</span> <span class="number">6</span>]</span><br><span class="line">************</span><br><span class="line">[[<span class="number">2</span>]</span><br><span class="line"> [<span class="number">5</span>]</span><br><span class="line"> [<span class="number">6</span>]]</span><br><span class="line">************</span><br><span class="line">[[<span class="number">5</span> <span class="number">6</span>]]</span><br></pre></td></tr></table></figure>
</li>
<li><p>tf.reshape()</p>
<p>函数的作用是将tensor变换为参数shape形式，其中的shape为一个列表形式，特殊的是列表可以实现逆序的遍历，即list(-1).-1所代表的含义是我们不用亲自去指定这一维的大小，函数会自动进行计算，但是列表中只能存在一个-1。（如果存在多个-1，就是一个存在多解的方程）。该函数有如下参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.reshape(tensor,shape,name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>官方给出的例子：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]</span></span><br><span class="line"><span class="comment"># tensor 't' has shape [9]</span></span><br><span class="line">reshape(t, [3, 3]) ==&gt; [[1, 2, 3],</span><br><span class="line">                        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor 't' is [[[1, 1], [2, 2]],</span></span><br><span class="line"><span class="comment">#                [[3, 3], [4, 4]]]</span></span><br><span class="line"><span class="comment"># tensor 't' has shape [2, 2, 2]</span></span><br><span class="line">reshape(t, [2, 4]) ==&gt; [[1, 1, 2, 2],</span><br><span class="line">                        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor 't' is [[[1, 1, 1],</span></span><br><span class="line"><span class="comment">#                 [2, 2, 2]],</span></span><br><span class="line"><span class="comment">#                [[3, 3, 3],</span></span><br><span class="line"><span class="comment">#                 [4, 4, 4]],</span></span><br><span class="line"><span class="comment">#                [[5, 5, 5],</span></span><br><span class="line"><span class="comment">#                 [6, 6, 6]]]</span></span><br><span class="line"><span class="comment"># tensor 't' has shape [3, 2, 3]</span></span><br><span class="line"><span class="comment"># pass '[-1]' to flatten 't'</span></span><br><span class="line">reshape(t, [-1]) ==&gt; [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]</span><br><span class="line"></span><br><span class="line"><span class="comment"># -1 can also be used to infer the shape</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -1 is inferred to be 9:</span></span><br><span class="line">reshape(t, [2, -1]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3],</span><br><span class="line">                         [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]</span><br><span class="line"><span class="comment"># -1 is inferred to be 2:</span></span><br><span class="line">reshape(t, [-1, 9]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3],</span><br><span class="line">                         [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]</span><br><span class="line"><span class="comment"># -1 is inferred to be 3:</span></span><br><span class="line">reshape(t, [ 2, -1, 3]) ==&gt; [[[1, 1, 1],</span><br><span class="line">                              [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                              [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]],</span><br><span class="line">                             [[<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>],</span><br><span class="line">                              [<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>],</span><br><span class="line">                              [<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor 't' is [7]</span></span><br><span class="line"><span class="comment"># shape `[]` reshapes to a scalar</span></span><br><span class="line">reshape(t, []) ==&gt; 7</span><br></pre></td></tr></table></figure>
</li>
<li><p>_getitem__方法</p>
<p>Python的方法<code>__getitem__</code> 可以让对象实现迭代功能，这样就可以使用<code>for...in...</code> 来迭代该对象了。比如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Animal</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, animal_list)</span>:</span></span><br><span class="line">        self.animals_name = animal_list</span><br><span class="line"></span><br><span class="line">animals = Animal([<span class="string">"dog"</span>,<span class="string">"cat"</span>,<span class="string">"fish"</span>])</span><br><span class="line"><span class="keyword">for</span> animal <span class="keyword">in</span> animals:</span><br><span class="line">    print(animal)</span><br></pre></td></tr></table></figure>

<p>在用 <code>for..in..</code> 迭代对象时，如果对象没有实现 <code>__iter__</code> <code>__next__</code> 迭代器协议，Python的解释器就会去寻找<code>__getitem__</code> 来迭代对象，如果连<code>__getitem__</code> 都没有定义，这解释器就会报对象不是迭代器的错误。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TypeError: <span class="string">'Animal'</span> object <span class="keyword">is</span> <span class="keyword">not</span> iterable</span><br></pre></td></tr></table></figure>

<p>如果在类中定义了<strong>getitem</strong>()方法，那么他的实例对象（假设为P）就可以这样P[key]取值。当实例对象做P[key]运算时，就会调用类中的<strong>getitem</strong>()方法。</p>
</li>
<li><p><strong>repr</strong>和<strong>str</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, value=<span class="string">'hello, world!'</span>)</span>:</span></span><br><span class="line">        self.data = value</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = Test()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">&lt;__main__.Test at <span class="number">0x7fa91c307190</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> t</span><br><span class="line">&lt;__main__.Test object at <span class="number">0x7fa91c307190</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看到了么？上面打印类对象并不是很友好，显示的是对象的内存地址</span></span><br><span class="line"><span class="comment"># 下面我们重构下该类的__repr__以及__str__，看看它们俩有啥区别</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重构__repr__</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestRepr</span><span class="params">(Test)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'TestRepr(%s)'</span> % self.data</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tr = TestRepr()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tr</span><br><span class="line">TestRepr(hello, world!)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> tr</span><br><span class="line">TestRepr(hello, world!)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重构__repr__方法后，不管直接输出对象还是通过print打印的信息都按我们__repr__方法中定义的格式进行显示了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重构__str__</span></span><br><span class="line">calss TestStr(Test):</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'[Value: %s]'</span> % self.data</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ts = TestStr()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ts</span><br><span class="line">&lt;__main__.TestStr at <span class="number">0x7fa91c314e50</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> ts</span><br><span class="line">[Value: hello, world!]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 你会发现，直接输出对象ts时并没有按我们__str__方法中定义的格式进行输出，而用print输出的信息却改变了</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>tensorflow中的tensorboard</p>
<p>深入讲解戳链接</p>
<p><a href="https://blog.csdn.net/qq_27825451/article/details/90229983" target="_blank" rel="noopener">https://blog.csdn.net/qq_27825451/article/details/90229983</a></p>
<p><img src="C:%5CUsers%5Cbme319%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200526201214488.png" alt="image-20200526201214488"></p>
<p>颜色浅的才是真实曲线，但真实曲线往往不好看，所以加入了平滑产生了颜色深的线，左侧有调节平滑的参数</p>
</li>
<li><p>keras保存模型和加载模型</p>
<p>需要用到save和load_model两个函数，举例如下：</p>
<p><strong>保存模型</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, LSTM, Embedding, Bidirectional</span><br><span class="line"><span class="keyword">from</span> keras_contrib.layers <span class="keyword">import</span> CRF</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras_contrib.layers.crf <span class="keyword">import</span> CRF, crf_loss, crf_viterbi_accuracy</span><br><span class="line"></span><br><span class="line"><span class="comment">#构造模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_lstm_crf_model</span><span class="params">(num_cates, seq_len, vocab_size, model_opts=dict<span class="params">()</span>)</span>:</span></span><br><span class="line">    opts = &#123;</span><br><span class="line">        <span class="string">'emb_size'</span>: <span class="number">256</span>,</span><br><span class="line">        <span class="string">'emb_trainable'</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">'emb_matrix'</span>: <span class="literal">None</span>,</span><br><span class="line">        <span class="string">'lstm_units'</span>: <span class="number">256</span>,</span><br><span class="line">        <span class="string">'optimizer'</span>: keras.optimizers.Adam()</span><br><span class="line">    &#125;</span><br><span class="line">    opts.update(model_opts)</span><br><span class="line"></span><br><span class="line">    input_seq = Input(shape=(seq_len,), dtype=<span class="string">'int32'</span>)</span><br><span class="line">    <span class="keyword">if</span> opts.get(<span class="string">'emb_matrix'</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        embedding = Embedding(vocab_size, opts[<span class="string">'emb_size'</span>],</span><br><span class="line">                              weights=[opts[<span class="string">'emb_matrix'</span>]],</span><br><span class="line">                              trainable=opts[<span class="string">'emb_trainable'</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        embedding = Embedding(vocab_size, opts[<span class="string">'emb_size'</span>])</span><br><span class="line">    x = embedding(input_seq)</span><br><span class="line">    lstm = LSTM(opts[<span class="string">'lstm_units'</span>], return_sequences=<span class="literal">True</span>)</span><br><span class="line">    x = Bidirectional(lstm)(x)</span><br><span class="line">    crf = CRF(num_cates, sparse_target=<span class="literal">True</span>)</span><br><span class="line">    output = crf(x)</span><br><span class="line"></span><br><span class="line">    model = Model(input_seq, output)</span><br><span class="line">    model.compile(opts[<span class="string">'optimizer'</span>], loss=crf.loss_function,</span><br><span class="line">                  metrics=[crf.accuracy])</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练，前面导入数据和处理数据省略</span></span><br><span class="line">model = build_lstm_crf_model(num_cates, seq_len=seq_len, vocab_size=vocab_size,</span><br><span class="line">                             model_opts=&#123;<span class="string">'emb_matrix'</span>: w2v_embeddings, <span class="string">'emb_size'</span>: <span class="number">100</span>, <span class="string">'emb_trainable'</span>: <span class="literal">False</span>&#125;)</span><br><span class="line">model.summary()<span class="comment"># 可以输出各层的参数状况</span></span><br><span class="line">tbCallBack = TensorBoard(log_dir=<span class="string">"./log"</span>)<span class="comment">#使用tensorBoard查看loss和accuracy曲线</span></span><br><span class="line">model.fit(train_X, train_y, batch_size=<span class="number">64</span>, epochs=<span class="number">10</span>, callbacks=[tbCallBack])<span class="comment">#训练</span></span><br><span class="line">model.save(<span class="string">'model.h5'</span>)<span class="comment">#保存模型，得安装相应模块，pip install h5py</span></span><br></pre></td></tr></table></figure>

<p><strong>加载模型</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line"></span><br><span class="line"><span class="comment">#CRF为自定义的层，load_model函数中需添加custom_objects参数，该参数接受一个字典，键值为自定义的层</span></span><br><span class="line">model=load_model(<span class="string">'model.h5'</span>,custom_objects=&#123;<span class="string">"CRF"</span>: CRF,<span class="string">"crf_loss"</span>: crf_loss, <span class="string">'crf_viterbi_accuracy'</span>: crf_viterbi_accuracy&#125;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>tf.estimator与tensorboard</p>
<p>一般情况下，在tensorflow中想使用tensorboard可视化参数和loss、accuracy等结果，需要配合session图结构一起（具体可参照许多博客，讲解较多）。但当在tensorflow中使用estimator进行训练、预测和验证时，会有所不同。</p>
<p>Estimator是一种可极大地简化机器学习编程的高阶 TensorFlow API，会封装很多流程，自动构建图。在使用Estimator的tf程序中，想要利用tensorboard进行可视化，只需要在model_fn中加入如下操作（以loss为例）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_fn</span><span class="params">(features, labels, mode, params)</span>:</span></span><br><span class="line">    ....</span><br><span class="line">    ....</span><br><span class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.TRAIN:</span><br><span class="line">            train_op = optimization.create_optimizer(</span><br><span class="line">                total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)</span><br><span class="line">            </span><br><span class="line">            tf.summary.scalar(<span class="string">'loss'</span>,total_loss)<span class="comment">#可视化loss</span></span><br><span class="line">                    </span><br><span class="line">            hook_dict = &#123;&#125;</span><br><span class="line">            hook_dict[<span class="string">'loss'</span>] = total_loss</span><br><span class="line">            hook_dict[<span class="string">'global_steps'</span>] = tf.train.get_or_create_global_step()</span><br><span class="line">            logging_hook = tf.train.LoggingTensorHook(</span><br><span class="line">                hook_dict, every_n_iter=FLAGS.save_summary_steps)</span><br><span class="line"></span><br><span class="line">            output_spec = tf.estimator.EstimatorSpec(</span><br><span class="line">                mode=mode,</span><br><span class="line">                loss=total_loss,</span><br><span class="line">                train_op=train_op,</span><br><span class="line">                training_hooks=[logging_hook])</span><br></pre></td></tr></table></figure>

<p>保存model的文件中会有日志文件生成，在终端启用tensorboard，定位到生成log文件的上一级文件夹目录后，执行如下语句即可（log就是日志目录名）</p>
<blockquote>
<p>tensorboard –logdir=log</p>
</blockquote>
</li>
<li><p>宏平均和微平均</p>
<p>针对多分类问题，预测任务时将会有多个混淆矩阵，宏平均是在各个混淆矩阵中分别计算查准率和查全率以及F1值，然后再计算平均值，微平均是将各个混淆矩阵的对应元素进行平均，得到TP、FP、TN、FN的平均值，再计算查准率和查全率以及F1值。</p>
<img src="https://i.loli.net/2020/06/09/Guq7CQB24zIfisv.png" style="zoom: 80%;" />



</li>
</ol>
<p><img src="https://i.loli.net/2020/06/09/MWSRa5PvUYiCrfu.png" style="zoom:80%;" /><img src="https://i.loli.net/2020/06/09/k2quzAgjyfY8s9n.png" alt=""></p>
<p><img src="https://i.loli.net/2020/06/09/k2quzAgjyfY8s9n.png" alt=""></p>
<p>关于两者的选择：</p>
<ul>
<li>若注重数据比例较大的类的影响，选择micro-average</li>
<li>若注重数据比较较小的类的影响，选择macro-average</li>
</ul>
<blockquote>
<p>如果每个class的样本数量相差不大，那么宏平均和微平均差异也不大</p>
<p>如果微平均远低于宏平均，则应该去检查样本量多 的class</p>
<p>如果宏平均远低于微平均，则应该去检查样本量少的class</p>
</blockquote>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>windows下jupyter notebook中切换虚拟环境</title>
    <url>/2020/11/12/jupyter%20notebook%E4%B8%AD%E5%88%87%E6%8D%A2%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/</url>
    <content><![CDATA[<p>命令行输入jupyter notebook后，ipynb的默认使用环境是系统中最原始的python版本，即使是在虚拟环境下打开的jupyter，若想要使用其他虚拟环境，需要在该虚拟环境下安装ipykernel包，命令如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install ipykernel</span><br></pre></td></tr></table></figure>

<p><img src="https://i.loli.net/2020/11/12/96vHq73CmRnEIQt.png" alt="image-20201112201451846"></p>
<p>然后将自己的环境添加到jupyter中，命令如下(venv是我的虚拟环境名称)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python -m ipykernel install --name venv</span><br></pre></td></tr></table></figure>

<p><img src="https://i.loli.net/2020/11/12/8ejxrvdlRmZkuog.png" alt="image-20201112201639860"></p>
<p>这样就可以在jupyter notebook里选择环境了！</p>
<p><img src="https://i.loli.net/2020/11/12/4kY7Gv3sX2AVQIy.png" alt="image-20201112201733891"></p>
<p>在jupyter里删除虚拟环境的命令</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">jupyter kernelspec remove venv</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>tip</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>综述：基于图的方法提取生物医学关系</title>
    <url>/2020/07/08/%E7%BB%BC%E8%BF%B0%EF%BC%9A%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%9A%84%E6%96%B9%E6%B3%95%E6%8F%90%E5%8F%96%E7%94%9F%E7%89%A9%E5%8C%BB%E5%AD%A6%E5%85%B3%E7%B3%BB/</url>
    <content><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>1、关系提取比命名实体识别要困难很多，accuracy不高；</p>
<p>2、关系提取和事件提取所用到的方法通常是一样的；</p>
<p>3、基于图的方法将生物医学概念转换为节点，将语法/语义链接转换为边 ；</p>
<p>4、生物医学领域一些知识库（有专家标注）：KEGG，STRING ，InterPro， InterDom，偏向临床的有PharmGKB， VARIMED， ClinVar；</p>
<h2 id="生物医学关系提取的应用"><a href="#生物医学关系提取的应用" class="headerlink" title="生物医学关系提取的应用"></a>生物医学关系提取的应用</h2><p>生物分子信息提取,  临床试验筛选 ,  药物基因组学 ,  诊断分类 , 药物不良反应 ， 药物之间相互作用 </p>
<p><img src="https://i.loli.net/2019/10/31/RMQgpCir5cPlK7e.png" alt=""></p>
<h3 id="生物分子信息提取（Biomolecular-information-extraction）"><a href="#生物分子信息提取（Biomolecular-information-extraction）" class="headerlink" title="生物分子信息提取（Biomolecular information extraction）"></a>生物分子信息提取（Biomolecular information extraction）</h3><p> 挖掘蛋白质-蛋白质相互作用(protein protein interaction, PPIs)、基因表型关联、基因本体论和通路信息 ， 涉及的方法中有许多采用NLP方法从文献中提取已知的疾病基因关系，然后用于预测新的疾病基因关系 。</p>
<h3 id="临床试验筛选（Clinical-trial-screening）"><a href="#临床试验筛选（Clinical-trial-screening）" class="headerlink" title="临床试验筛选（Clinical trial screening）"></a>临床试验筛选（Clinical trial screening）</h3><p> 临床试验在很大程度上以合格标准为特征，其中一些可以通过相关性来获得 （如no [diagnosis] for [rheumatoid arthritis] for at least [6 months]）， NLP支持已被证明在自动检测资格标准中的命名实体方面很有用，并进一步在提取命名实体之间的关系以描述资格标准方面很有用 。</p>
<h3 id="药物基因组学（Pharmacogenomics）"><a href="#药物基因组学（Pharmacogenomics）" class="headerlink" title="药物基因组学（Pharmacogenomics）"></a><strong>药物基因组学</strong>（Pharmacogenomics）</h3><p> 药物基因组学的目的是通过研究药物反应表型和患者遗传变异之间的关系来了解不同患者对药物的反应。  这些知识中的大部分可以从科学文献中挖掘出来，并整理到数据库中，以便发现新的关系。其中一个数据库是药物基因学研究网络和知识库 （PharmGKB ）， 最近的方法已经扩展到使用语义和句法分析以及统计机器学习工具来从科学文献和临床记录中挖掘有针对性的药物基因组学关系 。</p>
<h3 id="诊断分类（Diagnosis-categorization"><a href="#诊断分类（Diagnosis-categorization" class="headerlink" title="诊断分类（Diagnosis categorization)"></a>诊断分类（Diagnosis categorization)</h3><p>目前存在的已所开发的系统可以从电子病历(Electronic Medical Records, EMRs)中自动对诊断进行编码和分类。最近的工作证明了语义关系提取的成功，并将这些关系作为诊断分类的附加特征。</p>
<h3 id="药物不良反应和药物之间相互作用-（ADR-and-DDI）"><a href="#药物不良反应和药物之间相互作用-（ADR-and-DDI）" class="headerlink" title="药物不良反应和药物之间相互作用 （ADR and DDI）"></a>药物不良反应和药物之间相互作用 （ADR and DDI）</h3><p> ADR指的是服用药物造成的意外伤害。DDI是指一种药物同时影响另一种药物的活性。ADR是发病和死亡的重要原因，DDIs可能导致药物疗效降低或药物过量。检测潜在的ADR和DDIs可以指导药物的开发过程。越来越多的方法使用NLP从科学文献和临床记录中挖掘信息。这些系统经常探索药物、基因和通路之间的关系，发现文本中陈述的ADRs和DDIs。近年来的大量研究也探索了社交网络中用户生成的内容，以检测ADR。</p>
<h2 id="生物医学关系提取的通用方法"><a href="#生物医学关系提取的通用方法" class="headerlink" title="生物医学关系提取的通用方法"></a>生物医学关系提取的通用方法</h2><p><img src="https://i.loli.net/2019/10/31/lkqw72j9xMSheKm.png" alt=""></p>
<p>Section recognition：段落识别，区分不同段落标题下的文本，比如“主诉”，既往病史。</p>
<p>Sentence breaking： 断句，自动决定一个段落中句子的开头和结尾。 </p>
<p>Typographical/morphological analysis：提取大小写以及字母数字的字符的特征。</p>
<p>Stemming：词干提取，将单词还原为词根形式，如将performed变为perform。</p>
<p>POS tagging： 为句子中的每个单词指定词性标记 。</p>
<p>Parsing： 为一个句子分配句法结构，通常利用Stanford Parser获取constituency or dependency structure（两种语言结构）</p>
<p>typographical analysis，stemming，POS tagging和parsing的结果可以为共指关系和概念的识别提供特征。其中共指关系指的是 自动识别表示同一个实体的名词短语或代词，并将他们归类， 例如：新闻报道中的<strong>巴拉克－奥巴马</strong>这个实体，有的时候是<strong>美国总统</strong>，有的时候是<strong>奥巴马</strong>, 而有的时候是<strong>第44任美国总统</strong>，甚至有的时候是一个简单的代词<strong>他</strong>。当这些名词短语或代词出现在一起时，我们根据我们已有的知识或者是上下文信息都清楚地知道它们指代的是同一个实体。</p>
<p>graph mining在整个流程中扮演着一个中心概念的角色，它可以为连接局部特征（如tokens和词类标记）的方法提供一个收敛点，同时它的每一个分叉点代表着一个整合特征（如关系特征）被建立起来，最后它也是连接句法和语法特征的桥梁。</p>
<h2 id="生物医学关系提取的图表示和图算法"><a href="#生物医学关系提取的图表示和图算法" class="headerlink" title="生物医学关系提取的图表示和图算法"></a>生物医学关系提取的图表示和图算法</h2><p>算法主要分为几类：</p>
<p>1、识别概念对之间的最短路径（或其变体），可以用标准算法(如Dijkstra算法)来执行；</p>
<p>2、 创建关联图，然后尝试对其应用定制的标签 ；</p>
<p>3、 使用子图匹配来比较基于节点距离和边缘距离的子图之间的相似性; </p>
<p>4、 进行频繁子图或子树模式挖掘，直接提取候选关系 </p>
<p>5、 将图结构集成到解析器的学习目标中，直接解析句子中关系的图表示 </p>
<h2 id="生物医学关系提取的信息来源"><a href="#生物医学关系提取的信息来源" class="headerlink" title="生物医学关系提取的信息来源"></a>生物医学关系提取的信息来源</h2><h3 id="科学文献"><a href="#科学文献" class="headerlink" title="科学文献"></a>科学文献</h3><p>著名的community challenges：BioNLP shared tasks（事件的提取）、BioCreative shared tasks（蛋白质之间关系的提取）、DDIExtraction challenges 2011 and 2013（药物之间关系的提取）</p>
<p><img src="https://i.loli.net/2019/10/31/VrTi9tMSYhKac2D.png" alt=""></p>
<h3 id="临床叙事文本"><a href="#临床叙事文本" class="headerlink" title="临床叙事文本"></a>临床叙事文本</h3><p>挑战：</p>
<p>i2b2/VA-2010 challenge(i2b2—Informatics for Integrating Biology to the Bedside,VA—Veterans Association)：此挑战基于EMR文本数据，包括概念提取、assertion classification和关系分类</p>
<p>SemEval 2015 Task 14：disorder identification（无序识别） and disorder slot filling tasks（与BioNLP的事件提取很像，但是在临床的子领域）</p>
<h3 id="关系提取的共享资源"><a href="#关系提取的共享资源" class="headerlink" title="关系提取的共享资源"></a>关系提取的共享资源</h3><p><img src="https://i.loli.net/2019/10/31/aXhb1ulQW2JUGcm.png" alt=""></p>
<h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>引用的综述主要讨论基于图的算法，而笔者更着重于机器学习的算法，所以这篇博客对文章中的方法不做过多的描述，更多着眼于生物医学关系提取的一些自然语言处理方法和一些公开的数据集。</p>
<p>参考文献</p>
<p>[1] Luo Y , Özlem Uzuner, Szolovits P . Bridging semantics and syntax with graph algorithms-state-of-the-art of extracting biomedical relations[J]. Briefings in Bioinformatics, 2016, 18(1):160. </p>
]]></content>
      <categories>
        <category>文献阅读</category>
      </categories>
      <tags>
        <tag>Biomedical information extraction</tag>
      </tags>
  </entry>
  <entry>
    <title>python系列笔记1</title>
    <url>/2020/07/08/python%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B01/</url>
    <content><![CDATA[<h4 id="python语法笔记"><a href="#python语法笔记" class="headerlink" title="python语法笔记"></a>python语法笔记</h4><ol>
<li><p>Argparse中<strong>action</strong>的可选参数store_true和store_false</p>
<p>store_true 是指带触发action时为真，不触发则为假</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">parser.add_argument(<span class="string">'-c'</span>, action=<span class="string">'store_true'</span>)</span><br><span class="line"><span class="comment"># python test.py -c         =&gt; c是true（触发）</span></span><br><span class="line"><span class="comment"># python test.py             =&gt; c是false（无触发）</span></span><br></pre></td></tr></table></figure>



</li>
</ol>
<ol start="2">
<li><p>orderedDict</p>
<p>python中的字典是无序的，因为它是按照hash来存储的，但是python中有个模块collections(英文，收集、集合)，里面自带了一个子类OrderedDict，实现了对字典对象中元素的排序。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Regular dictionary"</span></span><br><span class="line">d=&#123;&#125;</span><br><span class="line">d[<span class="string">'a'</span>]=<span class="string">'A'</span></span><br><span class="line">d[<span class="string">'b'</span>]=<span class="string">'B'</span></span><br><span class="line">d[<span class="string">'c'</span>]=<span class="string">'C'</span></span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> d.items():</span><br><span class="line">    <span class="keyword">print</span> k,v </span><br><span class="line"><span class="keyword">print</span> <span class="string">"\nOrder dictionary"</span></span><br><span class="line">d1 = collections.OrderedDict()</span><br><span class="line">d1[<span class="string">'a'</span>] = <span class="string">'A'</span></span><br><span class="line">d1[<span class="string">'b'</span>] = <span class="string">'B'</span></span><br><span class="line">d1[<span class="string">'c'</span>] = <span class="string">'C'</span></span><br><span class="line">d1[<span class="string">'1'</span>] = <span class="string">'1'</span></span><br><span class="line">d1[<span class="string">'2'</span>] = <span class="string">'2'</span></span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> d1.items():</span><br><span class="line">    <span class="keyword">print</span> k,v</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">Regular dictionary</span><br><span class="line">a A</span><br><span class="line">c C</span><br><span class="line">b B</span><br><span class="line"></span><br><span class="line">Order dictionary</span><br><span class="line">a A</span><br><span class="line">b B</span><br><span class="line">c C</span><br><span class="line"><span class="number">1</span> <span class="number">1</span></span><br><span class="line"><span class="number">2</span> <span class="number">2</span></span><br></pre></td></tr></table></figure>



</li>
</ol>
<ol start="3">
<li><p>assert</p>
<p>Python assert（断言）用于判断一个表达式，在表达式条件为 false 的时候触发异常。比如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">assert</span> <span class="literal">True</span>     <span class="comment"># 条件为 true 正常执行</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">assert</span> <span class="literal">False</span>    <span class="comment"># 条件为 false 触发异常</span></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">AssertionError</span><br></pre></td></tr></table></figure>



</li>
</ol>
<ol start="4">
<li><p>import</p>
<p>可以直接导入另一个模块的函数或者变量。比如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a.py</span><br><span class="line">name=<span class="string">"zyy"</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_value</span><span class="params">()</span>:</span></span><br><span class="line">    d=&#123;&#125;</span><br><span class="line">    d[<span class="string">"zyy"</span>]=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">b.py</span><br><span class="line"><span class="keyword">from</span> a <span class="keyword">import</span> set_value,name</span><br></pre></td></tr></table></figure>



</li>
</ol>
<ol start="5">
<li><p>unicode</p>
<p>python内部使用Unicode编码，Unicode 是为了解决传统的字符编码方案的局限而产生的，它为每种语言中的每个字符设定了统一并且唯一的二进制编码，以满足跨语言、跨平台进行文本转换、处理的要求。</p>
</li>
<li><p>re.sub</p>
<p>替换字符串中的某些子串，可以用正则表达式来匹配被选子串。</p>
<p>re.sub(pattern, repl, string, count=0, flags=0)</p>
<p>pattern：表示正则表达式中的模式字符串；</p>
<p>repl：被替换的字符串（既可以是字符串，也可以是函数）；</p>
<p>string：要被处理的，要被替换的字符串；</p>
<p>count：匹配的次数, 默认是全部替换<br>比如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">st = <span class="string">"hello 2019"</span></span><br><span class="line">st = re.sub(<span class="string">"([0-9]+)"</span>,<span class="string">"danshengou"</span>,st)</span><br><span class="line">print(st)</span><br></pre></td></tr></table></figure>
</li>
<li><p>字典的items方法</p>
<p>Python 字典(Dictionary) items() 函数以<strong>列表</strong>返回可遍历的(键, 值) 元组数组。比如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"> </span><br><span class="line">dict = &#123;<span class="string">'Google'</span>: <span class="string">'www.google.com'</span>, <span class="string">'Runoob'</span>: <span class="string">'www.runoob.com'</span>, <span class="string">'taobao'</span>: <span class="string">'www.taobao.com'</span>&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">print</span> <span class="string">"字典值 : %s"</span> %  dict.items()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 遍历字典列表</span></span><br><span class="line"><span class="keyword">for</span> key,values <span class="keyword">in</span>  dict.items():</span><br><span class="line">    <span class="keyword">print</span> key,values</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出结果</span></span><br><span class="line">字典值 : [(<span class="string">'Google'</span>, <span class="string">'www.google.com'</span>), (<span class="string">'taobao'</span>, <span class="string">'www.taobao.com'</span>), (<span class="string">'Runoob'</span>, <span class="string">'www.runoob.com'</span>)]</span><br><span class="line">Google www.google.com</span><br><span class="line">taobao www.taobao.com</span><br><span class="line">Runoob www.runoob.com</span><br></pre></td></tr></table></figure>
</li>
<li><p>itertools 迭代器模块</p>
<ol>
<li><p>itertools.chain.from_iterable可以把多个可迭代对象组合起来，形成一个更大的迭代器。比如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Importing chain class from itertools </span></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> chain </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Single iterable containing iterable </span></span><br><span class="line"><span class="comment"># elements(strings) is passed as input </span></span><br><span class="line">from_iterable = chain.from_iterable([<span class="string">'geeks'</span>, </span><br><span class="line">									<span class="string">'for'</span>, </span><br><span class="line">									<span class="string">'geeks'</span>]) </span><br><span class="line"></span><br><span class="line"><span class="comment"># printing the flattened iterable </span></span><br><span class="line">print(list(from_iterable)) </span><br><span class="line"></span><br><span class="line"><span class="comment">#输出如下</span></span><br><span class="line">[‘g’, ‘e’, ‘e’, ‘k’, ‘s’, ‘f’, ‘o’, ‘r’, ‘g’, ‘e’, ‘e’, ‘k’, ‘s’]</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>cPickle.dump</p>
<p>cPickle可以对任意一种类型的python对象进行序列化操作，比如list，dict，甚至是一个类的对象等。而所谓的序列化，可理解就是为了能够完整的保存并能够完全可逆的恢复。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> cPickle</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = range(<span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cPickle.dump(data,open(<span class="string">"test/data.pkl"</span>,<span class="string">"wb"</span>))</span><br></pre></td></tr></table></figure>

<p>之后使用load函数可以恢复python对象</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = cPickle.load(open(<span class="string">"test/data.pkl"</span>,<span class="string">"rb"</span>))</span><br></pre></td></tr></table></figure>


</li>
</ol>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/07/08/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
      <categories>
        <category>Testing</category>
      </categories>
      <tags>
        <tag>标签</tag>
      </tags>
  </entry>
  <entry>
    <title>EliIE:一个符合临床试验合格标准的开源信息提取系统</title>
    <url>/2020/07/08/EliIE%EF%BC%9A%E4%B8%80%E4%B8%AA%E7%AC%A6%E5%90%88%E4%B8%B4%E5%BA%8A%E8%AF%95%E9%AA%8C%E5%90%88%E6%A0%BC%E6%A0%87%E5%87%86%E7%9A%84%E5%BC%80%E6%BA%90%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><h3 id="临床试验合格标准和规范化"><a href="#临床试验合格标准和规范化" class="headerlink" title="临床试验合格标准和规范化"></a>临床试验合格标准和规范化</h3><p>目前的一些规范化的工作需要耗费许多人力去设定句法规则以及符合要求的语法概念，而且这些工作的结果很多缺乏语义互操作性。</p>
<p>OMOP CDM具有语义互操作性， 它通过使用通用信息模型和相关临床实体(如条件、观察和药物)的多个标准术语对数据进行标准化，从而支持不同的观察数据库之间的互操作性。但目前将EHR数据转为标准化的OMOP数据仍需手动操作，无法对大规模的数据快速操作。</p>
<p>这篇文章构建了一个系统，该系统能够让计算机自动地庞大的EHR数据进行标准化处理，且使得各个观察性数据库彼此之间可相互操作。</p>
<h3 id="生物医学信息提取的自然语言处理方法"><a href="#生物医学信息提取的自然语言处理方法" class="headerlink" title="生物医学信息提取的自然语言处理方法"></a>生物医学信息提取的自然语言处理方法</h3><p>Biomedical information extraction（Biomedical IE）主要包括三个子课题：（1）命名实体识别(NER)；（2）名字实体之间提取二元关系，如症状和治疗的关系、蛋白质间的相互关系；（3）事件抽取，即在提取的概念中找到复杂的关系，比如基因调控关系。</p>
<p>Biomedical IE的方法大概有以下五种：基于规则的方法、基于知识的方法、基于统计学的方法、基于学习的方法以及混合的方法。CRFs和SVMs均属于基于学习的方法。i2b2（一个挑战中的数据集）上NER最好的结果是F1 score 0.85，关系提取最好的结果是0.74。</p>
<p>词嵌入和深度学习技术在生物医学信息提取提取上也有很大的发展，但这些方法，包括上述所提到i的，都只被用于文献文本和EHRs中的临床notes，而没有被用于临床研究的EC自由文本中（<strong>到底有什么区别，目前还不是很懂？？</strong>）</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="数据集和标注"><a href="#数据集和标注" class="headerlink" title="数据集和标注"></a>数据集和标注</h3><p>从Clinical-Trials.gov上选择230例阿尔兹海默症的临床试验，提取其中的“eligibility criteria”部分的文本信息，进行标注，文本大小从100个words到超过1000个words不等。一个临床医生和两个学生对文本进行实体和属性的标注，并规定标注的指南。</p>
<p>实体类型：condition,observation, drug/substance, and procedure or device，每个实体包含四类属性：modifiers/qualifiers,temporal constraints, measurements, and anatomic location。实体和属性之前也有关系，但是是单向的，如“modified by,” “has value”。所有的标注工作在一个软件（Brat）上进行，最终有8008个实体，3550个属性和3529个关系，下图为软件上的标注示例。</p>
<p><img src="https://i.loli.net/2019/11/01/AE7xcDIZadSzuH9.png" alt=""></p>
<h2 id="ElilE的系统结构"><a href="#ElilE的系统结构" class="headerlink" title="ElilE的系统结构"></a>ElilE的系统结构</h2><p><img src="https://i.loli.net/2019/11/01/pEyxG6v4kAeht1a.png" alt=""></p>
<h3 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h3><p>EC文本中过滤掉一些在EHR筛选中用不到的规则，如知情同意书和患者意愿书。</p>
<h3 id="Phase-1-CRF进行序列标注"><a href="#Phase-1-CRF进行序列标注" class="headerlink" title="Phase 1: CRF进行序列标注"></a>Phase 1: CRF进行序列标注</h3><p>实体和属性的标注一起进行，包括7个类别（4类实体和3类属性），使用BIO标签进行标注，如下所示。</p>
<p><img src="https://i.loli.net/2019/11/01/lHou7IiGqe3M649.png" alt=""></p>
<h3 id="Phase-2-否定检测"><a href="#Phase-2-否定检测" class="headerlink" title="Phase 2: 否定检测"></a>Phase 2: 否定检测</h3><p> 否定检测对于判断一个标准是用于包含还是用于排除非常重要。 该篇文章采用NegEx算法，在标注集中每个语句都被打上了肯定或者否定的标签。</p>
<h3 id="Phase-3-关系提取"><a href="#Phase-3-关系提取" class="headerlink" title="Phase 3: 关系提取"></a>Phase 3: 关系提取</h3><p>在识别出的术语中进行关系提取（实体和属性之间的关系），采用基于径向基函数的SVM算法。</p>
<p>下图为序列标注时用到的特征和关系提取时的标注准则。</p>
<p><img src="https://i.loli.net/2019/11/01/zK3hvAUp7TcrMCx.png" alt=""></p>
<h3 id="Phase-4-概念标准化和输出结构化"><a href="#Phase-4-概念标准化和输出结构化" class="headerlink" title="Phase 4: 概念标准化和输出结构化"></a>Phase 4: 概念标准化和输出结构化</h3><p>参照OMOP CDM</p>
<h2 id="评价准则"><a href="#评价准则" class="headerlink" title="评价准则"></a>评价准则</h2><p>亘古不变的precision、recall和F1 score。即对每个任务进行评价，又对整体的端到端的结果进行评价。</p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>数据集的一些描述性统计结果如下。</p>
<p><img src="https://raw.githubusercontent.com/liuqiangh/picGo/master/img/20191101155747.png" alt=""></p>
<p>通过实验，确定训练集的大小</p>
<p><img src="https://i.loli.net/2019/11/01/XZlFuHtLQxWjyN9.png" alt=""></p>
<p>实体和属性的识别结果如下，总计了许多方法，包括CRF的选取的不同特征，其他文献中的方法等等。</p>
<p><img src="https://i.loli.net/2019/11/01/RJfaHimtWMdQZsz.png" alt=""></p>
<p>关系提取的结果表现如下。</p>
<p><img src="https://i.loli.net/2019/11/01/fqOBPCvWjIRc4GD.png" alt=""></p>
<p>整个系统的评价结果如下。</p>
<p><img src="https://i.loli.net/2019/11/01/q9pDYmRvS81deOW.png" alt=""></p>
<h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><p>1、文章讨论了该系统的五个优势，笔者认为五个中最重要的一个就是该系统可实现多个任务，是一个综合性的生物医学信息提取系统，而目前很多基于机器学习的方法都只能实现一个任务。</p>
<p>（但这篇文章因为有多个子任务，所以对每个子任务讨论研究得都不够透彻，基本上用的是一些传统的方法，细节度不够。）</p>
<p>2、对文本进行标注时指南不够细致，对概念的颗粒度没有明确要求。</p>
<p>3、泛化性能差，可能对其他领域的疾病不适用。</p>
<p>4、实体和实体之间（而不是属性）没有提取到关系。</p>
]]></content>
      <categories>
        <category>文献阅读</category>
      </categories>
      <tags>
        <tag>Biomedical information extraction</tag>
      </tags>
  </entry>
</search>
