<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="文献阅读," />








  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico?v=5.1.2" />






<meta name="description" content="原文链接：http:&#x2F;&#x2F;gvv.mpi-inf.mpg.de&#x2F;projects&#x2F;FML&#x2F;本文由马克斯-普朗克研究所和斯坦福大学等机构合作完成，是 CVPR 2018 的 oral 文章。为了提升单张图片重建 3D 脸部模型的效果，该论文采用了多层次的脸部结构重建方法，作者把传统的基于参数化 3D 可变形模型（3DMM）作为基础模型，在此之上引入纠正模型来增加模型的表达力。实验表明纠正模型使得 3">
<meta name="keywords" content="文献阅读">
<meta property="og:type" content="article">
<meta property="og:title" content="Self-supervised Multi-level Face Model Learning for Monocular Reconstruction at over 250 Hz">
<meta property="og:url" content="https:&#x2F;&#x2F;gsynf.github.io&#x2F;2019&#x2F;04&#x2F;16&#x2F;2019-04-16-Self-supervised%20Multi-level%20Face%20Model%20Learning%20for%20Monocular%20Reconstruction%20at%20over%20250%20Hz&#x2F;index.html">
<meta property="og:site_name" content="Gsynf | My Blog">
<meta property="og:description" content="原文链接：http:&#x2F;&#x2F;gvv.mpi-inf.mpg.de&#x2F;projects&#x2F;FML&#x2F;本文由马克斯-普朗克研究所和斯坦福大学等机构合作完成，是 CVPR 2018 的 oral 文章。为了提升单张图片重建 3D 脸部模型的效果，该论文采用了多层次的脸部结构重建方法，作者把传统的基于参数化 3D 可变形模型（3DMM）作为基础模型，在此之上引入纠正模型来增加模型的表达力。实验表明纠正模型使得 3">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Gsynf&#x2F;BlogImg&#x2F;master&#x2F;20190415173416703.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;17&#x2F;5cb7363754822.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;17&#x2F;5cb740d84cec8.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;23&#x2F;5cbe6c2b369e2.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;23&#x2F;5cbe6ddb18d95.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;23&#x2F;5cbe6dffa30a5.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;23&#x2F;5cbe6e98b5929.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;23&#x2F;5cbe6f230022f.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;23&#x2F;5cbe6f741286a.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;23&#x2F;5cbe6f8977b2b.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;23&#x2F;5cbe6ffe4a354.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;23&#x2F;5cbe701dae588.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;23&#x2F;5cbe7048cd56a.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;19&#x2F;5cb92b2795684.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;18&#x2F;5cb81c54f2c3b.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;18&#x2F;5cb81ec0f13c8.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;18&#x2F;5cb81f54052d0.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;18&#x2F;5cb8214fdef38.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;18&#x2F;5cb82cffc7cd2.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;18&#x2F;5cb8655472c5e.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;18&#x2F;5cb8670434ec8.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;18&#x2F;5cb86e96cfd60.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;18&#x2F;5cb86ebd7b546.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;19&#x2F;5cb924aa97869.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;19&#x2F;5cb9297b56673.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;23&#x2F;5cbe712d1ccdf.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;23&#x2F;5cbe716a8a397.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;04&#x2F;23&#x2F;5cbe719973ff2.png">
<meta property="og:updated_time" content="2019-10-22T14:37:49.985Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Gsynf&#x2F;BlogImg&#x2F;master&#x2F;20190415173416703.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://gsynf.github.io/2019/04/16/2019-04-16-Self-supervised Multi-level Face Model Learning for Monocular Reconstruction at over 250 Hz/"/>





  <title>Self-supervised Multi-level Face Model Learning for Monocular Reconstruction at over 250 Hz | Gsynf | My Blog</title>
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>
    <a href="https://github.com/Gsynf" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Gsynf | My Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">曾梦想仗剑天涯，后来学习忙没去</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://gsynf.github.io/2019/04/16/2019-04-16-Self-supervised%20Multi-level%20Face%20Model%20Learning%20for%20Monocular%20Reconstruction%20at%20over%20250%20Hz/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Gsynf">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/me.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gsynf | My Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Self-supervised Multi-level Face Model Learning for Monocular Reconstruction at over 250 Hz</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-16T00:00:00+08:00">
                2019-04-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计</span>
                
                <span title="字数统计">
                  8.4k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长</span>
                
                <span title="阅读时长">
                  29
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>原文链接：<a href="http://gvv.mpi-inf.mpg.de/projects/FML/" target="_blank" rel="noopener">http://gvv.mpi-inf.mpg.de/projects/FML/</a><br>本文由马克斯-普朗克研究所和斯坦福大学等机构合作完成，是 CVPR 2018 的 oral 文章。<br>为了提升单张图片重建 3D 脸部模型的效果，该论文采用了多层次的脸部结构重建方法，作者把传统的基于参数化 3D 可变形模型（3DMM）作为基础模型，在此之上引入纠正模型来增加模型的表达力。实验表明纠正模型使得 3D 脸部重建效果更接近原图，而且能重建出更多细节。<br>基础模型与纠正模型均为线性模型，其中基础模型的基向量通过对训练样本做 PCA 得到（即 3DMM 模型），而纠正模型的基向量由神经网络直接学习得到。脸部的形状与纹理通过基础模型加上纠正模型来拟合。算法使用编码器来学习基础模型和纠正模型的组合参数。随后整合两个模型的结果，通过解码器得到渲染的 3D 脸部模型。然后，算法把 3D 模型成像，对比成像结果与输入图片的差异，目标是使差异变小，因此该方法是自监督的方式进行训练。此外，算法还限制了成像结果与输入图的脸部特征点要对齐。注意该方法中只有编码器是可学习的，而解码器和渲染器都是手工设计的可导层，不是可学习的。为了让模型更加鲁棒和训练过程更加稳定，作者在损失函数上加入额外调节项，用于提升纠正模型的平滑性、纹理的稀疏性和整体一致性。</p>
</blockquote>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><img src="https://raw.githubusercontent.com/Gsynf/BlogImg/master/20190415173416703.png" alt="figure0"><br>我们的新型单目重建方法高质量地估计了面部几何形状，皮肤反射率(包括面部毛发)和超过250赫兹的入射光。利用前馈反渲染网络，学习了一种可训练的多层人脸表示方法。端到端训练基于自我监督的损失，不需要密集的地面实况。</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p><em>从一张单一的图像中重建密集的人脸几何和外观三维模型是非常具有挑战性和不适定的。为了约束该问题，许多方法依赖于强先验，如从有限的三维扫描数据中学习的参数化人脸模型。然而，先验的模型限制了人脸几何、皮肤反射率和光照的真实多样性的泛化。为了解决这一问题，我们提出了一种联合学习方法1)人脸形状、表情、反射率和光照的回归器的方法。2)基于并行学习的参数化人脸模型。我们的多层人脸模型结合了3D 可变形模型（3DMM）的正则化优势和学习校正空间的空间外泛化。我们的多层人脸模型结合了三维形态模型的正则化优势和学习校正空间的空间外泛化。我们通过在多层细节级别上定义的专家设计的可微渲染器来融合卷积编码器，以及自监督训练损失，在没有密集注释的自然环境图像上进行端到端的训练。我们的方法在重建质量上优于目前的技术水平，可以更好地推广到真实世界的人脸，运行频率超过250赫兹。</em></p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h1><p>在过去的几十年里，单目人脸重建在计算机视觉和图形学领域引起了极大的关注。我们的目标是从一张照片中估计出一个高质量的个性化人脸模型。这种模型理想地包括几个可解释的语义维度，例如，三维人脸形状和表情以及表面反射率特性。这一领域的研究是由不断增加的人脸图像可用性所推动的，例如，家用摄像头捕捉的人脸图像，以及跨多个领域的广泛重要应用，如面部运动捕捉、游戏和电影的内容创建、虚拟和增强现实以及通信。</p>
<p>从一张照片中重建人脸是一个非常具有挑战性和不适定的逆问题，因为图像的形成过程将多个复杂的物理维度(几何、反射率和光照)卷积成每个像素的单一颜色测量。为了解决这种不适定性，研究人员已经做了额外的先验假设，比如将人脸限制在一个低维子空间中，例如，3D可变形模型(3DMM)从有限尺寸的扫描数据库中学习。许多最先进的基于优化的和基于学习的人脸重建方法严重依赖这些先验。虽然这些算法产生了令人印象深刻的结果，但它们不能很好地推广到除受限低维子空间之外的底层模型。因此，重建的三维人脸可能缺乏重要的面部细节，包含不正确的面部特征，并不能很好地与图像匹配。例如，纯合成数据训练的算法或使用3DMM进行正则化的算法的重构质量的对胡须显示效果会急剧下降。一些方法试图通过启发式来防止这些失败，例如，一个单独的分割方法来消除分离皮肤和头发区域带来的的歧义。最近的方法通过添加精细尺度的细节，或者基于阴影的形状，或者基于预先学习的回归量来细化拟合的先验。然而，这些方法依赖于缓慢的优化，或者需要高质量的带注释的训练语义库。此外，他们没有为中型形状、反射率和动画建立一个的改进子空间，这对泛化来说至关重要。最近，Sela等人的预测了一个单像素深度图，通过在训练中学习去变形和填补有一个有限几何子空间的洞。虽然结果令人印象深刻，但非刚性匹配是离线运行的。此外，他们的方法只捕捉人脸的几何形状，如果人脸与训练语义库有很大差异，就会失败，例如考虑皮肤反射率和面部毛发。理想情况下，人们希望构建更好的先验，用有意义的和可解释的参数解释各种各样的现实世界面孔。用传统方法学习这样的模型需要大量标记密集的真实世界数据，这实际上是不可行的。</p>
<p>我们提出了一种全新的端到端可训练的方法，该方法联合学习1)一个有效的回归因子来估计高质量的身份几何、面部表情和有色皮肤反射率，以及2)一个改进的多级人脸模型的参数化，该模型能更好地概括和解释现实世界中的人脸多样性。我们的方法可以在稀疏标记的自然环境图像上进行端到端的训练，并在超过250赫兹的单目RGB输入中重建人脸和光照。我们的方法利用3DMM进行正则化，并利用学习的校正空间进行空间外泛化。为了使自然环境图像的端到端训练成为可能，我们提出了一种混合卷积自编码器，它将CNN编码器与专家设计的可微渲染层和自监督损失结合起来，两者都定义在多个细节级别。此外，我们加入了一个新的轮廓约束，生成更好的人脸匹配。与Tewari等人的不同，我们的自动编码器学习了一个改进的多级模型，超越了预先定义的低维参数脸部先验。实验结果表明，该方法具有较强的鲁棒性、较好的泛化性，并能较好地估计几何形状、反射率和光照质量。</p>
<h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2.Related Work"></a>2.Related Work</h1><p>我们主要讨论基于优化和学习的方法，这些方法使用参数模型。而高质量的多视图三维重建方法已经存在，我们感兴趣的是更难的单眼重建问题。</p>
<p><strong>参数化人脸模型:</strong>应用最广泛的人脸模型是3D可变形模型(3DMM)，这是一种通过高质量扫描学习人脸几何和纹理的仿射参数化人脸模型。【5】中提出了一个相似的人脸动画模型。最近，Booth等人从大约10,000个面部扫描中创建了一个大规模的面部模型(LSFM)，这代表了一个更丰富的形状分布。Booth等人的【9】中，人脸模型被一个“in-the-wild”纹理模型增强。将这种模型拟合到图像上是一个非凸优化问题，类似于基于Active Shape (ASMs) 和<br>Appearance (AAMs) 模型的结构。虽然3DMMs是一种高效的先验，但它们将人脸重构限制在一个有限的低维子空间内，如胡须或特征鼻子无法重构。相反，我们通过共同学习一个修正模型来扩展有限的子空间，该模型可以更好地概括真实数据。</p>
<p><strong>基于优化的方法：</strong>单目人脸重建、基于图像采集的重建以及高质量三维人脸平台的估计，很多方法都是基于能量优化的。从不同的数据源，如照片集、网络照片、视频，得到了令人印象深刻的人脸重建结果。此外，还提出了不依赖于训练过的形状或外观模型的方法，如使用模态分析得到的模型或利用视觉流与消息处理相结合。虽然实时人脸跟踪在一般情况下是可行的，但是基于优化的人脸重建在计算上是昂贵的。此外，基于优化的方法对初始化很敏感，需要2D特征点检测。一些方法允许3D人脸轮廓在预定义的路径上(例如等值线)滑动或迭代固定顶点集，以此来寻找三维轮廓对应。我们的方法既不需要昂贵的优化策略，也不需要参数初始化，但通过在训练过程中考虑轮廓，它可以精确地将3D人脸网格与图像匹配。</p>
<p><strong>基于学习的方法：</strong>除了基于优化的重构方法外，还有许多基于学习的方法。其中，基于卷积神经网络或受限玻尔兹曼机的方法用来学习在图像中高精度地监测基准点。此外，我们还可以发现(弱)监督的深度网络，它集成生成模型来解决像面部表现捕捉这样的任务。<br>Ranja等人提出了一种多用途CNN，用于从人脸图像中恢复语义参数(如年龄、性别、姿势)。理查森等人提出了一种基于学习和优化的混合方法，可以从一张图像重建精细的面部几何形状。在【48】中提出了训练端到端回归器，以恢复粗糙和精细尺度的面部几何形状。在【61】中，对人脸形状和纹理进行回归，进行人脸识别。后一种人脸重建方法对真实世界人脸多样性的推广受到底层低维人脸模型的限制。</p>
<p><strong>基校正和子空间学习：</strong>通过添加中等尺度的细节，可以提高人脸重建的质量。Li等人使用增量PCA对表情基实现实时个性化。Bouaziz等人【11】提出了基于流形谐波的中尺度形状校正方法。最近，Garrido等人提出了一种基于固定校正基础的单目视频学习中尺度形状的方法。Sela等人的【53】直接提出回归深度和单像素对应，从而超越了3DMM受限子空间。然而，它们不能恢复彩色表面反射率，并且需要离线非刚性配准步骤才能获得已知一致拓扑结构的重构。据我们所知，目前还没有一种算法能从自然环境图像中同时学习几何和反射率校正。</p>
<p><strong>生成模型的深度集成：</strong>这是Jaderberg等人的开创性工作【31】，引入了空间转换网络，实现了一个神经网络中的位置不变性。透视转换网络【67】能够从单个二维图像中获得三维对象表示。gvvn库【27】实现了用于这种转换的低级计算机视觉层。最近，一种基于模型的人脸自动编码器(MoFA)【59】被提出用于单目人脸重建，它结合了专家设计的渲染层和可训练的CNN编码器。它们的结果是显著的，但仅限于人脸模型的固定低维子空间。外子空间的变异，如面部细节和个性化的鼻子，没有被重现，严重降低了重建的质量。我们的方法解决了所有这些挑战，在几何和反射率方面实现了更强的鲁棒性和更高的质量。</p>
<h1 id="3-Method-Overview"><a href="#3-Method-Overview" class="headerlink" title="3.Method Overview"></a>3.Method Overview</h1><p>我们全新的人脸重建方法从一个单一的图像高质量地估计几何形状，皮肤反射率和入射光。我们在同时学习多级参数化人脸模型的基础上，联合训练各维度的回归器，如图1所示。<br><img src="https://i.loli.net/2019/04/17/5cb7363754822.jpg" alt="figure1"><br><em>图1所示。我们的方法在超过250赫兹时回归一个低维的潜在人脸表示。前馈CNN是与一个超越当前3DMMs低维子空间的多层次人脸模型共同学习的。可训练图层显示为蓝色，专家设计的图层显示为灰色。训练是基于可微的图像形成，结合自监督损失(橙色)。</em></p>
<p><strong>参数回归：</strong>在测试时(图1，左)，使用一个前馈CNN计算一个低维的，但有表现力和鉴别力，潜在的空间人脸表示花费在4ms以下。如AlexNet【37】或VGG-Face【44】。我们的潜在空间是基于一个新的多层次的人脸模型(第4节)它结合了一个粗糙尺度的3DMM与可训练的单顶点几何和皮肤反射校正。这使我们的方法能够超越有限的低维几何和皮肤反射子空间，通常使用基于3DMM的方法进行人脸拟合。</p>
<p><strong>自监督训练：</strong>我们训练(图1，右)前馈网络和校正空间，基于一种新的CNN架构，不依赖于密集标注的基础几何事实、皮肤反射率和光照训练语义库。为此，我们将多层模型与专家设计的图像形成层(第5节)相结合，得到一个可微计算机图形模块。为了实现多级人脸模型的联合估计，该模块提出了粗糙3DMM模型和中等规模的模型，其中均包括校正。在训练方面，我们使用自监督的损失函数(第6节)，使我们的架构能够在大量自然环境人脸图像上进行有效的端到端训练，而不需要密集地注释基础事实。我们定性和定量地评估我们的方法，并将其与最先进的优化和基于学习的面部重建技术进行比较(第7节)。</p>
<h1 id="4-Trainable-Multi-level-Face-Model"><a href="#4-Trainable-Multi-level-Face-Model" class="headerlink" title="4.Trainable Multi-level Face Model"></a>4.Trainable Multi-level Face Model</h1><p>我们的方法的核心是一个新的多层次的面部模型，确定面部几何形状和皮肤反射率的参数。我们的模型是基于一个流形模板网格与N ~ 30k顶点和每个顶点的皮肤反射率。我们将所有顶点v<sub>i</sub>∈V的x-、y-和z-坐标叠加在一个几何向量v<sup>f</sup>∈R<sup>3n</sup>中，几何与反射率参数化表示如下:<br><img src="https://i.loli.net/2019/04/17/5cb740d84cec8.png" alt="公式1,2"></p>
<p>在基本级别上是参数化面（粗糙）部几何vb和(粗糙)皮肤反射rb通过低维的一组参数(α;β)。<br>此外,我们使用校正加入中等规模的几何Fg和反射率Fr变形,参数化的(δg;Θg)将基准面模型与校正模型结合起来，就得到最终的水平模型，参数化vf和rf。下面，我们将描述多级人脸模型的不同级别。</p>
<h2 id="4-1-Static-Parametric-Base-Model"><a href="#4-1-Static-Parametric-Base-Model" class="headerlink" title="4.1. Static Parametric Base Model"></a>4.1. Static Parametric Base Model</h2><p>底层采用的参数化人脸模型通过两个独立的仿射模型表达了似是而非的人脸几何和反射率空间:<br><img src="https://i.loli.net/2019/04/23/5cbe6c2b369e2.png" alt="公式3,4"><br>反射率变化的子空间由向量b张成，该向量由PCA从200个高质量人脸扫描数据集中创建。<br>几何子空间分为Ms和Me，表示形状和表达式的变化。<br>这些向量是用PCA从[2]和[17]的混合形状子集中生成的。注意，这些混合形状已经使用变形转移[56]转移到我们的拓扑结构中。基捕获了使用的混合形状的99%的方差。我们使用ms = mr = 80个形状和反射向量，me = 64个表达向量。相关的标准差σg和σr。</p>
<h2 id="4-2-Trainable-Shape-and-Reflectance-Corrections"><a href="#4-2-Trainable-Shape-and-Reflectance-Corrections" class="headerlink" title="4.2. Trainable Shape and Reflectance Corrections"></a>4.2. Trainable Shape and Reflectance Corrections</h2><p>许多基于优化和学习的重建技术，如[7,5,60，59]。由于其低维性，4.1节所述的基本模型对面部形状和反射率的高精度建模表达能力有限。一个特殊的问题是皮肤反照率的变化，因为所使用的模型有种族偏见，缺乏面部毛发，例如胡须。<br>这项工作的目的是通过学习一个可训练的纠正模型来改进这一点，该模型可以表示这些空间外的变化。与其他使用固定的预定义校正基础[25]的方法不同，我们学习了校正器的生成模型和最佳校正参数。此外，我们不需要对几何形状、皮肤反射率和入射光照的地面真相注释。<br>与基准水平的关键区别在于，修正水平不使用固定的预训练基础，而是直接从训练数据中学习生成模型和系数。</p>
<h1 id="5-Differentiable-Image-Formation-Model"><a href="#5-Differentiable-Image-Formation-Model" class="headerlink" title="5. Differentiable Image Formation Model"></a>5. Differentiable Image Formation Model</h1><p>为了训练我们新颖的端到端多层人脸重建方法，我们需要一个可微的图像形成模型。下面，我们将描述它的组件。<br><strong>全参相机:</strong>这一部分是求了相机的内参数，相机模型包含内部物理并执行透视划分。<br><strong>光照模型:</strong>利用球谐基函数，建立了远距离照明的假设，并对入射光进行了近似<img src="https://i.loli.net/2019/04/23/5cbe6ddb18d95.png" alt=""><br>我们假设入射光只依赖于表面法向量n:<br><img src="https://i.loli.net/2019/04/23/5cbe6dffa30a5.png" alt="公式5"><br>其中，圈点表示哈达玛乘积，r表示表面反射，B代表球谐基函数带，rb是控制照明的系数。<br>由于入射光足够平滑，平均误差低于1%，可以实现只有B = 3波段独立于照明。这导致了每个颜色通道的变量<img src="https://i.loli.net/2019/04/23/5cbe6e98b5929.png" alt=""><br><strong>图像合成：</strong>我们的可微图像生成层以模型空间的顶点形状和反射率作为输入。这可以是基本级别模型rb和vb，或者最终级别模型vf和rf，其中包括所学习的校正器。<img src="https://i.loli.net/2019/04/23/5cbe6f230022f.png" alt="">表示基面第i个顶点的位置和反射率(l = b)和最后一层(‘l= f)。<br>我们的渲染层接受这些信息，并形成一个基于点的场景渲染，如下所示。<br>首先，它将这些点映射到相机空间，即<img src="https://i.loli.net/2019/04/23/5cbe6f741286a.png" alt="">)然后计算所有顶点的投影像素位置为<img src="https://i.loli.net/2019/04/23/5cbe6f8977b2b.png" alt="">这些像素位置的阴影颜色c ‘ i是根据前面描述的光照模型计算的:<img src="https://i.loli.net/2019/04/23/5cbe6ffe4a354.png" alt="">其中，n是相关联的摄像机空间法线到v. 我们的图像形成模型是可微的，这使得端到端的训练使用反向传播。回归器学习预测的自由变量有: 模型参数<img src="https://i.loli.net/2019/04/23/5cbe701dae588.png" alt="">相机参数R, t和照明参数γ。此外,在训练期间,我们学习纠正形状和反射率基地,Θr。这导致了以下未知向量:<br><img src="https://i.loli.net/2019/04/23/5cbe7048cd56a.png" alt=""></p>
<p><img src="https://i.loli.net/2019/04/19/5cb92b2795684.png" alt="figure2"><br><em>图2所示。我们将固定和移动特征点区分开来。这将使得更好的轮廓对齐。注意外部轮廓如何依赖于刚性头部姿态(左)。在全局反射率恒常性约束中使用了皮肤掩模(右)。</em></p>
<h1 id="6-Self-supervised-Learning"><a href="#6-Self-supervised-Learning" class="headerlink" title="6.Self-supervised Learning"></a>6.Self-supervised Learning</h1><p>我们的人脸回归网络使用一个全新的自监督损失来进行训练，这样使得它能够符合我们的基础模型并且端到端地学习每个像素的校正。我们的损失包括数据拟合以及正则项：<br><img src="https://i.loli.net/2019/04/18/5cb81c54f2c3b.png" alt="公式6"><br>这里E<sub>date</sub>惩罚了模型在输入图像上的偏差，E<sub>reg</sub>对人脸在粗尺度和中等尺度上的先验假设进行编码，ω<sub>reg</sub>是控制正则项的平衡因子。数据拟合项基于稀疏和稠密的一致性约束。<br><img src="https://i.loli.net/2019/04/18/5cb81ec0f13c8.png" alt="公式7"><br>正则化项表示在基础模型和修正模型上的先验假设:<br><img src="https://i.loli.net/2019/04/18/5cb81f54052d0.png" alt="公式8"><br>在下文中，将详细介绍各个部分。</p>
<h2 id="6-1-数据项"><a href="#6-1-数据项" class="headerlink" title="6.1 数据项"></a>6.1 数据项</h2><p><strong>多维稠密光度损失：</strong>我们采用了一个稠密多维光度损失函数，用来衡量粗尺度上的偏差并且更好地拟合输入。V是所有可见顶点的集合，我们的光度项可以定义为：<br><img src="https://i.loli.net/2019/04/18/5cb8214fdef38.png" alt="公式9"><br>这里u<sup>l</sup><sub>i</sub>（x）是屏幕的空间位置，c<sup>l</sup><sub>i</sub>（x）是第i个顶点的阴影颜色，L是当前训练的图像。为了增强鲁棒性，我们使用了l<sub>2,1</sub>-norm，其中l<sub>2</sub>-norm来衡量颜色之间的差距，但是对每个像素的l<sub>2</sub>-norm进行求和促进了稀疏性，因为它对应于l<sub>1</sub>-norm。使用后脸剔除可以计算可见性，这是一个近似值，不过效果很好，因为脸部可以近似看作一个凸面体。</p>
<p><strong>稀疏特征点：</strong>面部包含许多突出的特征点，我们通过一个弱监督自动地检测66个面部标记。面部标记点集可以分为两类：固定的和移动的特征点。固定的特征点，例如眼睛和鼻子，和模板模型上的固定顶点相关联；移动的特征点，例如面部轮廓，基于刚性位姿而改变在模板中的位置，见图2（右），模型可以显式地表达为：<br><img src="https://i.loli.net/2019/04/18/5cb82cffc7cd2.png" alt="公式10"><br>这里，k<sub>f</sub>是目标顶点的索引：对于固定的点，我们将对应网格顶点的索引进行硬编码（把一个本来应该写到配置信息中的信息直接在程序代码中写死），移动特征点的索引通过例如交替方案来计算。在每一步的随机梯度下降中，我们发现网格顶点是最靠近3D线的，通过相机中心和检测到的2D特征点的背投影来定义。我们计算欧式距离的平方并且将k<sub>r</sub>设为最近顶点的索引。</p>
<h2 id="6-2-正则化项"><a href="#6-2-正则化项" class="headerlink" title="6.2 正则化项"></a>6.2 正则化项</h2><p><strong>统计正则化：</strong>我们对基础水平的3DMM模型参数进行统计正则化，以确保重构的合理性。基于模型参数服从零均值高斯分布的假设，我们采用Tikhonov正则化:<br><img src="https://i.loli.net/2019/04/18/5cb8655472c5e.png" alt="公式11"><br>这是在不适定的单目重建场景中防止人脸几何形状和反射率退化的常见约束条件。</p>
<p><strong>平滑性校正：</strong>我们还通过对所有顶点集合V中的顶点位移添加拉普拉斯正则项来增加局部光滑性:<br><img src="https://i.loli.net/2019/04/18/5cb8670434ec8.png" alt="公式12"><br>其中（F<sub>g</sub>(x)）<sub>i</sub>=(F<sub>g</sub>(δ<sub>g</sub>|θ<sub>g</sub>))<sub>i</sub>表示第i个顶点给定参数x的的修正量，并且N<sub>i</sub>是第i个顶点的一环领域。</p>
<p><strong>局部反射稀疏性：</strong>根据最近的内在分解方法，我们加强实施稀疏性，以进一步规范全重构的反射性：<br><img src="https://i.loli.net/2019/04/18/5cb86e96cfd60.png" alt="公式13"><br>其中，<img src="https://i.loli.net/2019/04/18/5cb86ebd7b546.png" alt="">是衡量输入中颜色之间色度相似性的恒权值，这里X<sup>old</sup>是在前一轮迭代中的估值参数。我们假设具有相同色度的像素更有可能具有相同的反射率。l<sub>2,p</sub>-norm对组合反射率估计值添加稀疏性。在所有实验中我们假定α=50，p=0.9。</p>
<p><strong>全局反射恒常性：</strong>我们在一组只覆盖皮肤区域的固定顶点上设定皮肤反射率恒定，见图2（右）<br><img src="https://i.loli.net/2019/04/19/5cb924aa97869.png" alt="公式14"><br>这里，M是每个像素的皮肤掩模，G<sub>i</sub>存储着掩模区域顶点索引的六个随机采样。这样做是为了保证整个皮肤区域有相同的反射率。为了更高效，我们假设同一块皮肤区域中任意一对顶点之间的反射率都近似相同。需要注意的是，这些区域可能有面部的毛发，这是不包含在掩模中的。当组合在一起的时候，局部和全局的反射从反射通道中高效的移除了阴影。</p>
<p><strong>稳定性：</strong>我们还通过强制执行小的顶点位移来确保修正后的几何形状接近于基础重构:<br><img src="https://i.loli.net/2019/04/19/5cb9297b56673.png" alt="公式15"></p>
<h1 id="7-Results"><a href="#7-Results" class="headerlink" title="7.Results"></a>7.Results</h1><p>我们演示了前馈编码器的联合端到端自监督训练和基于野生图像的新型多层次人脸表示，而不需要密集注释。我们的方法是在250赫兹以上的高质量的位置、形状、表达式、反射和照明进行回归，见图3。<br><img src="https://i.loli.net/2019/04/23/5cbe712d1ccdf.png" alt="figure3"><br>对于前馈编码器，我们使用了Alexnet[37]的修改版本，该版本输出我们的人脸模型的参数。请注意，可以使用其他前馈架构。<br>我们使用caffe[32]实现了我们的方法。培训基于批量大小为5的adadelta。我们对我们的网络进行预培训，使其达到20万次迭代的基本水平，学习率为0:01。之后，我们对整个网络进行了190k次迭代的微调，基本级别的学习率为0:001，几何结构的学习率为0:005，反射校正的学习率为0:01。我们的网络的所有组件都在CUDA[43]中实现，以便进行有效的培训，这需要16个小时。我们在所有的实验中都使用常数w。在下面，我们将校正参数的大小c固定为500，用于几何和反射率。我们测试了不同的校正空间（线性和非线性），见图5。线性校正基础给出了最好的结果，因此我们将其用于所有后续实验。详情请参阅补充文件。<br><img src="https://i.loli.net/2019/04/23/5cbe716a8a397.png" alt="figure5"><br>我们的方法是在一个没有密集注释的原始的的面部图像的语料库上训练的。我们结合了四种不同的数据集：Celeba[41]、LFW[28]、Face-Warehouse[16]和300-VW[18、54、62]。稀疏地标注释自动获得[52]，我们使用HAAR级联人脸检测[13]裁剪到240240像素的紧面边界框。检测不良的图像会根据地标置信度自动删除。总的来说，我们使用144K图像，随机分为一组训练（142K图像）和验证（2K图像）。<br>我们将我们的最终输出（“最终”）与从预训练网络获得的基础低维3dmm重建（“基础”）进行比较，以说明我们的多级模型允许我们恢复更高质量的几何体和反射比（图4）。下面，我们将展示更多的结果，评估我们的方法，并与最新技术进行比较。<br><img src="https://i.loli.net/2019/04/23/5cbe719973ff2.png" alt="figure4"></p>
<h2 id="7-1-与最新技术的比较"><a href="#7-1-与最新技术的比较" class="headerlink" title="7.1 与最新技术的比较"></a>7.1 与最新技术的比较</h2><p><strong>基于最优化的技术：</strong>我们将其与基于优化的高质量重建方法进行了比较。[25]，见图6。我们的方法获得了相似的几何质量，但由于我们学习了纠正空间，更好地捕捉了人的特征。由于我们的方法共同学习了一个校正反射空间，它可以离开底层3dmm的受限子空间，从而产生更真实的外观。注意，与Garrido等人不同，我们的方法在测试时不需要标志，运行速度更快（4ms对120s每幅图像）。我们还比较了布斯等人的方法。[9]见图7。我们的方法共同学习一个更好的形状和反射模型，而他们的方法只建立一个’原始图片模型，包含阴影。与我们的方法不同，布斯等人基于优化，需要初始化或标志。<br><strong>基于学习的技术：</strong>我们比较了Tewari等人基于学习的高质量重建方法。[59]（图8），Richardson等人[47，48]（图9）和Sela等人〔53〕（图9）。在使用的合成训练语料库或使用的3dmm模型范围内，这些方法获得了令人印象深刻的结果，但受到了子空间形状和反射变化的影响，例如有胡子的人。我们的方法不仅对面部毛发和化妆都很有效，而且可以根据共同学习的模型自动学习重建这些变化。重建需要4 ms，而[53]需要缓慢的离线非刚性配准，以从预测深度图获得无孔重建。此外，我们还共同获得了彩色反射和照明的重建。由于我们的模型学习，我们的方法能够离开三维空间的低维空间，这导致了一个更现实的面部外观和几何重建。</p>
<h2 id="7-2-数量级上的结果"><a href="#7-2-数量级上的结果" class="headerlink" title="7.2 数量级上的结果"></a>7.2 数量级上的结果</h2><p>我们定量地评估了我们的方法。对于几何图形，我们使用FaceWarehouse[16]数据集并重建180个网格（9个标识，每个表达式20个）。我们比较了不同的方法，在对齐后（刚性变换加上各向同性缩放），以提供的地面真相使用豪斯多夫距离。我们的方法优于Tewari等人的基于学习的技术。[59]和Kim Etal.〔36〕见Tab。1。我们接近Garrido等人的高质量优化方法。[25]虽然速度快了几个数量级（4ms vs.120sec），但在测试时不需要进行特征检测，见图10（上图）。[16]主要包含“干净”的脸，没有化妆或胡须，因为这会导致问题，即使是高质量的离线3D重建方法。我们的兴趣是坚定地处理这种更困难的情况，在这种情况下，我们证明我们的方法明显优于以前的方法，见图。8, 6和9。我们还评估了我们的方法，在一个视频序列（300帧）具有挑战性的表达和特征面，这是在3dmm的跨度之外。Valgaerts等人获得了基本事实。〔63〕。我们的校正方法（平均值：1.77mm，标准差：0.29mm）显著优于基础结果（平均值：2.16mm，标准差：0.29mm），Garrido等人[25]在粗（平均：1.96mm，标准差：0.35mm）和中等（平均：1.97mm，标准差：0.41mm）水平，以及Tewari等人[59]（平均值：2.94mm，标准差：0.28mm），见图10（底部）。我们在验证集上评估我们方法的光度拟合误差，见图11。我们的最终结果（平均值：0.072，标准差：0.020）与基础水平（平均值：0.092，标准差：0.025）相比，误差（RGB空间中的距离，通道在[0；1]中）显著降低。</p>
<h1 id="8-局限性"><a href="#8-局限性" class="headerlink" title="8.局限性"></a>8.局限性</h1><p>我们在250Hz以上的频率下进行了高质量的单目重建，即使是在有面部毛发的情况下，或是在有挑战性的脸上。不过，我们的方法有一些局限性，可以在将来的工作中加以解决：外部遮挡（例如，通过眼镜）被烘烤到我们的校正中，见图12。解决这一问题需要对训练语料库进行语义分割。不能保证封闭面区域的一致重建。对于鲁棒模型学习，我们强制纠正空间的低维性。因此，我们无法恢复细尺度的表面细节。我们认为这是一个正交的研究方向，已经产生了令人印象深刻的结果[47，48，53]。</p>
<h1 id="9-总结"><a href="#9-总结" class="headerlink" title="9.总结"></a>9.总结</h1><p>我们提出了第一种共同学习人脸模型和参数回归器的方法，用于人脸形状、表情、外观和照明。它结合了3dmm正则化的优点和已知校正空间的超空间泛化。这克服了目前依赖强先验的方法的缺点，提高了泛化性和鲁棒性，并导致250Hz以上的高质量重建。在这项工作中，我们将重点放在人脸重建上，我们的方法不局限于人脸，因为它可以推广到更多的对象类。因此，我们认为这是从原始图像构建三维模型的第一步。</p>
<p>以上。</p>
<p><strong><em>注</em></strong>：转载文章请注明出处，谢谢~</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" rel="tag"># 文献阅读</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/10/2019-04-10-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_6/" rel="next" title="数据结构_6">
                <i class="fa fa-chevron-left"></i> 数据结构_6
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/22/2019-04-22-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_7/" rel="prev" title="数据结构_7">
                数据结构_7 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/me.jpg"
               alt="Gsynf" />
          <p class="site-author-name" itemprop="name">Gsynf</p>
           
              <p class="site-description motion-element" itemprop="description">顺风不浪，逆风不怂！形而上学，不行退学！</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">89</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">29</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/Gsynf" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="mailto:hupengfei@nssc.ac.cn" target="_blank" title="E-Mail">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                    
                      E-Mail
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Abstract"><span class="nav-number">2.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Introduction"><span class="nav-number">3.</span> <span class="nav-text">1.Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-Related-Work"><span class="nav-number">4.</span> <span class="nav-text">2.Related Work</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-Method-Overview"><span class="nav-number">5.</span> <span class="nav-text">3.Method Overview</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-Trainable-Multi-level-Face-Model"><span class="nav-number">6.</span> <span class="nav-text">4.Trainable Multi-level Face Model</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-Static-Parametric-Base-Model"><span class="nav-number">6.1.</span> <span class="nav-text">4.1. Static Parametric Base Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-Trainable-Shape-and-Reflectance-Corrections"><span class="nav-number">6.2.</span> <span class="nav-text">4.2. Trainable Shape and Reflectance Corrections</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-Differentiable-Image-Formation-Model"><span class="nav-number">7.</span> <span class="nav-text">5. Differentiable Image Formation Model</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-Self-supervised-Learning"><span class="nav-number">8.</span> <span class="nav-text">6.Self-supervised Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#6-1-数据项"><span class="nav-number">8.1.</span> <span class="nav-text">6.1 数据项</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-2-正则化项"><span class="nav-number">8.2.</span> <span class="nav-text">6.2 正则化项</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#7-Results"><span class="nav-number">9.</span> <span class="nav-text">7.Results</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#7-1-与最新技术的比较"><span class="nav-number">9.1.</span> <span class="nav-text">7.1 与最新技术的比较</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-2-数量级上的结果"><span class="nav-number">9.2.</span> <span class="nav-text">7.2 数量级上的结果</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#8-局限性"><span class="nav-number">10.</span> <span class="nav-text">8.局限性</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#9-总结"><span class="nav-number">11.</span> <span class="nav-text">9.总结</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2017 - 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Gsynf</span>
</div>



        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
